{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32444efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 : 18.50118634521492\n",
      "\n",
      "b0 : 963.191336472558\n",
      "\n",
      "b: \n",
      "Intercept    963.191336\n",
      "roe           18.501186\n",
      "dtype: float64\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PyGraphs/Example-2-3-3.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_t/76fj8y2n4k31mxsw5z3j0rtm0000gn/T/ipykernel_3013/837085488.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'salary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PyGraphs/Example-2-3-3.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m## d 2.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3003\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_pdf.py\u001b[0m in \u001b[0;36mprint_pdf\u001b[0;34m(self, filename, dpi, bbox_inches_restore, metadata)\u001b[0m\n\u001b[1;32m   2716\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdfFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2719\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_file_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_opened\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PyGraphs/Example-2-3-3.pdf'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 28 15:06:56 2022\n",
    "\n",
    "@author: zoeyroberts\n",
    "\"\"\"\n",
    "\n",
    "### part 1 \n",
    "\n",
    "## a 2.1\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "x = ceosal1['roe']\n",
    "y = ceosal1['salary']\n",
    "\n",
    "# ingredients to the OLS formulas:\n",
    "cov_xy = np.cov(x,y)[1,0] # access 2. row and 1. column of covariance matrix\n",
    "var_x = np.var(x, ddof=1)\n",
    "x_bar = np.mean(x)\n",
    "y_bar = np.mean(y)\n",
    "\n",
    "# manual calculation of OLS\n",
    "b1 = cov_xy / var_x\n",
    "b0 = y_bar - b1 * x_bar\n",
    "print(f'b1 : {b1}\\n')\n",
    "print(f'b0 : {b0}\\n')\n",
    "\n",
    "## b 2.2\n",
    "import wooldridge as woo \n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "## c 2.3\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# scatter plot and fitted values:\n",
    "plt.plot('roe','salary', data=ceosal1, color='grey', marker='o', linestyle='')\n",
    "plt.plot(ceosal1['roe'], results.fittedvalues, color='black', linestyle='-')\n",
    "plt.ylabel('salary')\n",
    "plt.xlabel('roe')\n",
    "plt.savefig('PyGraphs/Example-2-3-3.pdf')\n",
    "\n",
    "## d 2.6\n",
    "import pandas as pd\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# obtain predicted values and residuals:\n",
    "salary_hat = results.fittedvalues\n",
    "u_hat = results.resid\n",
    "\n",
    "# Wooldridge, Table 2.2:\n",
    "table = pd.DataFrame({'roe': ceosal1['roe'],\n",
    "                      'salary': ceosal1['salary'],\n",
    "                      'salary_hat': salary_hat,\n",
    "                      'u_hat': u_hat})\n",
    "print(f'table.head(15): \\n{table.head(15)}\\n')\n",
    "\n",
    "## e 2.9\n",
    "import pandas as pd\n",
    "\n",
    "vote1 = woo.dataWoo('vote1')\n",
    "\n",
    "#OLS regression:\n",
    "reg = smf.ols(formula='voteA ~ shareA', data=vote1)\n",
    "results = reg.fit()\n",
    "\n",
    "# print results using summary:\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table:\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "## f 2.13\n",
    "import numpy as np\n",
    "\n",
    "meap93 = woo.dataWoo('meap93')\n",
    "\n",
    "# estimate the model and save the results as \"results\":\n",
    "reg = smf.ols(formula='math10 ~ lnchprg', data=meap93)\n",
    "results = reg.fit()\n",
    "\n",
    "# number of obs.:\n",
    "n = results.nobs\n",
    "\n",
    "# SER: \n",
    "u_hat_var = np.var(results.resid, ddof=1)\n",
    "SER = np.sqrt(u_hat_var) * np.sqrt((n - 1) / (n - 2))\n",
    "print(f'=SER: {SER}\\n')\n",
    "\n",
    "# SE of b0 & b1, respectively:\n",
    "lnchprg_sq_mean = np.mean(meap93['lnchprg'] ** 2)\n",
    "lnchprg_var = np.var(meap93['lnchprg'], ddof=1)\n",
    "b1_se = SER / (np.sqrt(lnchprg_var)\n",
    "               * np.sqrt(n-1)) * np.sqrt(lnchprg_sq_mean)\n",
    "b0_se = SER / (np.sqrt(lnchprg_var) * np.sqrt(n-1))\n",
    "\n",
    "## g 3.7\n",
    "import patsy as pt\n",
    "\n",
    "gpa1 = woo.dataWoo('gpa1')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(gpa1)\n",
    "k = 2\n",
    "\n",
    "# extract y:\n",
    "y = gpa1['colGPA']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const' : 1, 'hsGPA': gpa1['hsGPA'], 'ACT': gpa1['ACT']})\n",
    "\n",
    "# alternative with patsy: \n",
    "y2, X2 = pt.dmatrices('colGPA ~ hsGPA + ACT', data=gpa1, return_type='dataframe')\n",
    "\n",
    "# display first rows of X: \n",
    "print(f'X.head(): \\n{X.head()}\\n')\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# residuals, estimated vairance of u and SER: \n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "##### part 2\n",
    "\n",
    "#### chapter 2\n",
    "\n",
    "### 3\n",
    "import numpy as np\n",
    "\n",
    "student = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "gpa = np.array([2.8, 3.4, 3.0, 3.5, 3.6, 3.0, 2.7, 3.7])\n",
    "act = np.array([21, 24, 26, 27, 29, 25, 25, 30])\n",
    "\n",
    "df = pd.DataFrame({'student' : student,\n",
    "                   'gpa' : gpa,\n",
    "                   'act' : act})\n",
    "\n",
    "y = df['gpa']\n",
    "X = pd.DataFrame({'const' : 1, 'act': df['act']})\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(8, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "reg = smf.ols(formula='gpa ~ act', data=df)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table:\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "\n",
    "## (i) B0 is equal to 0.568, which can be interpreted as the GPA earned prediction for students who got a 0 on the ACT\n",
    "##     this is unlikely, but could still be useful as a 'base' GPA\n",
    "##     if an ACT score rises by 5 points, GPA should rise by .511 points (ceteris paribus), which is logical\n",
    "## (ii) the residuals sum to 0.067, which is very close to 0\n",
    "## (iii) predicted GPA is 2.6121\n",
    "## (iv) R^2 of this model (the percentage of variation in GPA explained by ACT scores) is equal to 0.577, or 57.7%\n",
    "\n",
    "### c1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TaxData = woo.dataWoo('401k')\n",
    "    \n",
    "#  prate                       participation rate, percent\n",
    "#  mrate                       401k plan match rate\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = TaxData.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = TaxData.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors\n",
    "n = len(TaxData)\n",
    "k = 1\n",
    "\n",
    "# scater plot prate vs mrate\n",
    "plt.plot(TaxData['prate'], TaxData['mrate'], 'o', color='black')\n",
    "plt.ylabel('Participaation Rate %')\n",
    "plt.xlabel('401k Match rate')\n",
    "plt.savefig('Graph-401k.pdf')\n",
    "plt.close()\n",
    "\n",
    "# extract y\n",
    "y = TaxData['prate']\n",
    "\n",
    "# extract X & add a column of ones\n",
    "X = pd.DataFrame({'const': 1, 'mrate': TaxData['mrate']})\n",
    "\n",
    "# alternative with patsy:\n",
    "y2, X2 = pt.dmatrices('prate ~ mrate', data=TaxData, return_type='dataframe')\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "reg = smf.ols(formula='prate ~ mrate', data=TaxData)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) the average participation rate is 87.36 and the average match rate is 0.73\n",
    "## (ii) b0 is 83.08, b1 is 5.86, and r-squared is 0.075\n",
    "## (iii) b0 means that if one has a 401k plan match rate of 0, they will still have a participation rate of at least 83.08\n",
    "##       b1, the coefficient on mrate, implies an increase in mrate by 1 unit will increase prate by 5.86\n",
    "## (iv) the predicted prate when mrate is 3.5 is 103.59; this is not a reasonable preidction because hypothetically participation rate could not exceed 100%\n",
    "## (v) 7.5% of the variance in prate is explained by mrate, which is quite low \n",
    "\n",
    "### c3\n",
    "sleep75 = woo.dataWoo('sleep75')\n",
    "\n",
    "y = sleep75['sleep']\n",
    "X = pd.DataFrame({'const' : 1, 'totwrk': sleep75['totwrk']})\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(706, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "reg = smf.ols(formula='sleep ~ totwrk', data=sleep75)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "## (i) sleep = 3586.3770 - 0.1507*totwrk, n = 706, R^2 = 0.013\n",
    "##     this intercept implies that people who work 0 hours a week (the dream) sleep for 3586 minutes per week\n",
    "## (ii)iIf total work increases by 2 hours (120 minutes), sleep will fall by 18.084 minutes; this is not a very significant effect\n",
    "\n",
    "### c6\n",
    "testdata = woo.dataWoo('MEAP93')\n",
    "  \n",
    "# expend                   expend. per stud., $\n",
    "# math10                   perc studs passing MEAP math\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = testdata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = testdata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(testdata)\n",
    "k = 1\n",
    "\n",
    "# scater plot math10 vs expend\n",
    "plt.plot(testdata['expend'], testdata['math10'], 'o', color='black')\n",
    "plt.ylabel('math passing rate %')\n",
    "plt.xlabel('expenditure /student $')\n",
    "plt.savefig('graph-testdata.pdf')\n",
    "plt.close()\n",
    "\n",
    "# extract y:\n",
    "y = testdata['math10']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'expend': testdata['expend']})\n",
    "\n",
    "# alternative with patsy:\n",
    "y2, X2 = pt.dmatrices('math10 ~ expend', data=testdata, return_type='dataframe')\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='math10 ~ expend', data=testdata)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# estimate log-level model for math10 and logexpend\n",
    "reg = smf.ols(formula='math10 ~ np.log(expend)', data=testdata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) diminishing effect seems more approriate because only 100% of students can pass, so eventually the slope would get less steep as it approaches a 100% passing rate given the law of diminishing marginal returns\n",
    "## (ii) ∆math10/∆expend = b1/expend so ∆expend/expend 0.1 and then math10 = (∆expend/expend)*b1 = 0.1*b1; therefore, b1/10 is the percentage point change in math10 given a 10% increase in expend\n",
    "## (iii) estimated equation is math10 = -69.34 + 11.16*log(expend) + u, the sample size is 408, and r-squared is 0.30\n",
    "## (iv) is spending increases by 10%, then the estimated percentage point increase in math10 is 11.16\n",
    "## (v) -69.34 + 11.16log(expend) > 100\n",
    "##     log(expend) > 15.17 \n",
    "##     therefore, the equation must meet the condiction of expend > 3,874,782.15 for math10 to be above 100, which is unlikely\n",
    "\n",
    "### c8\n",
    "import scipy.stats as stats\n",
    "\n",
    "np.random.seed(1234567)\n",
    "n = 500\n",
    "beta0 = 1\n",
    "beta1 = 2\n",
    "su = 6\n",
    "\n",
    "x = stats.uniform.rvs(0,10, size=n)\n",
    "u = stats.norm.rvs(0,su, size=n)\n",
    "y = beta0 + beta1 * x + u\n",
    "\n",
    "x_mean = np.mean(x)\n",
    "print(x_mean)\n",
    "\n",
    "x_std = np.std(x)\n",
    "print(x_std)\n",
    "\n",
    "u_mean = np.mean(u)\n",
    "print(u_mean)\n",
    "\n",
    "u_std = np.std(u)\n",
    "print(u_std)\n",
    "\n",
    "df=pd.DataFrame({'y' : y, 'x' : x})\n",
    "reg = smf.ols(formula='y ~ x', data=df)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "u_hat = results.resid\n",
    "u_hat_mean = np.mean(u_hat)\n",
    "print(u_hat_mean)\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "n = 500\n",
    "beta0 = 1\n",
    "beta1 = 2\n",
    "su = 6\n",
    "\n",
    "x = stats.uniform.rvs(0,10, size=n)\n",
    "u = stats.norm.rvs(0,su, size=n)\n",
    "y = beta0 + beta1 * x + u\n",
    "\n",
    "x_mean = np.mean(x)\n",
    "print(x_mean)\n",
    "\n",
    "x_std = np.std(x)\n",
    "print(x_std)\n",
    "\n",
    "u_mean = np.mean(u)\n",
    "print(u_mean)\n",
    "\n",
    "u_std = np.std(u)\n",
    "print(u_std)\n",
    "\n",
    "df=pd.DataFrame({'y' : y, 'x' : x})\n",
    "reg = smf.ols(formula='y ~ x', data=df)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "## (i) sample mean = 5.0346 and sample standard deviation = 2.9465\n",
    "## (ii) sample mean of u is 0.05, which is slightly above 0; this is expected because the sample is not perfectly normally distributed\n",
    "##      even though most of the values are centered around 0, it is not exact\n",
    "##      sample standard deviation of u is 5.734\n",
    "## (iii) the intercept was estimated as 1.23, slightly above 1\n",
    "##       the beta1 was estimated as 1.96, which is slightly below the true value of 2\n",
    "##       these sample esimates are different from the population true values because the sample is only a small subset of the total population, and samples cannot be predetermined to be perfectly representative\n",
    "## (iv) the expected value of the residuals is effectively zero, meaning the first assumption of OLS holds\n",
    "## (v) the expected value of the sampled u was slightly larger, at 0.05, but it is still very close to 0\n",
    "## (vi) the new intercept is 0.65, and the new beta1 is 2.06. They are different again because individual samples differ from populations slightly \n",
    "\n",
    "### c11\n",
    "gpadata = woo.dataWoo('GPA1')\n",
    "\n",
    " # colGPA                  MSU GPA\n",
    " # PC                      =1 of pers computer at sch\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = gpadata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = gpadata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(gpadata)\n",
    "k = 1\n",
    "\n",
    "# scater plot colGPA vs PC\n",
    "plt.plot(gpadata['colGPA'], gpadata['PC'], 'o', color='black')\n",
    "plt.ylabel('college gpa')\n",
    "plt.xlabel('owns a pc')\n",
    "plt.savefig('graph-colgpa.pdf')\n",
    "plt.close()\n",
    "\n",
    "# extract y:\n",
    "y = gpadata['colGPA']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'PC': gpadata['PC']})\n",
    "\n",
    "# alternative with patsy:\n",
    "y2, X2 = pt.dmatrices('colGPA ~ PC', data=gpadata, return_type='dataframe')\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "reg = smf.ols(formula='colGPA ~ PC', data=gpadata)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) there are 141 students in the sample; the average college gpa is 3.06 while the high is 4.00\n",
    "## (ii) 39.72% of students own their own PC\n",
    "## (iii) b0 is 2.99 and b1 is 0.17; b0 is relatively high meaning the base gpa someone can have without owning a pc is 2.99 and b1 is low, so owning pc does not increase one's gpa substantially \n",
    "## (iv) r-squared is 0.05, which means 5%, very little, of the variation is college gpa is explained by owning a pv\n",
    "## (v) this does not imply owning a pc has a causal effect on college gpa becuase the two variables only have a minor correlation according to the estimated equation \n",
    "\n",
    "#### chapter 3\n",
    "\n",
    "### 3\n",
    "## (i) the sign of B1 will be negative if there is a trade off between sleep and work\n",
    "## (ii)  people with more education will sleep less, due to possibly longer work hours and the sign of B3 might be positive, as older people might have more stable schedules\n",
    "## (iii) if someone works 5 more hours a week, their total sleep for the week will fall by 44.4 minutes; this is not a large change when considered over a whole week of sleep\n",
    "## (iv) the coefficient on B3 implies that for each further year of education attained, a person will sleep 11.13 fewer minutes per week. This is the largest coefficient, but because education has a capped value, it probably is not making a huge difference on total sleep amounts\n",
    "## (v) no, the R^2 is only .113, which is quite low. People might have different preferences for length of sleep, or they could have children, which greatly lowers one's time sleeping. I do not think that these would be correlated with time working\n",
    "\n",
    "### 4\n",
    "lawdata = woo.dataWoo('LAWSCH85')\n",
    "\n",
    "## (i) we expect b5 to be ≤ 0 because a higher ranking implies a number closer to 1 not 100, and one would expect a higher salary having gone to a better law school\n",
    "## (ii) we expect a positive slope for b1, b2, b3, and b4 a higher lsat score, gpa, number of volumes in the law school library, cost of annual attendance implies the individual is attending a better law school and therefore should have a higher starting salary post graduating\n",
    "## (iii) ceretis paribus, a median GPA difference by one point increases one's starting salary by 24.8%\n",
    "## (iv) a 1% increase in the number of volumes in the law school library increases one's starting salary by 3.8%\n",
    "## (v) i would say it is better to attend a higher ranked law school because a difference in ranking of 20 points is worth a 7% increase in one's starting salary\n",
    "\n",
    "### 6\n",
    "## y = B0 + B1x1 + B2x2 + B3x3 + u\n",
    "\n",
    "## (i) E(B1hat) = B1, E(B2hat) = B2\n",
    "##     E(theta) = E(B1hat + B2hat) = E(B1hat) + E(B2hat) = B1 + B2\n",
    "##     therefore, theta = B1 + B2 and theta is an unbiased estimator of theta\n",
    "## (ii) Var(theta) = Var(B1hat + B2hat) = Var(B1hat) + Var(B2hat) + 2Cov(B1hat, B2hat)\n",
    "##      Var(theeta) = Var(B1hat) + Var(B2hat) + 2Corr(B1hat, B2hat)*sigmaB1*sigmaB2\n",
    "\n",
    "### 7\n",
    "## heteroskedasticity, endogeneity, and multicollinearity can all cause an OLS estimator to be biased\n",
    "\n",
    "### 10\n",
    "## (i) would expect B~ and B^ to be relatively similar; as x1 is highly correlated with x2 and x3, their effects on B are likely to be similar\n",
    "##     this implies that the estimated models would be similar as well\n",
    "## (ii) would expect B~ and B^ to be different in this case, as x2 and x3 are adding new information to the model\n",
    "## (iii) standard errors would likely increase with the addition of variables, escpecially ones with low explanatory power\n",
    "## (iv) effect of standard errors would be ambivalent, as additional variables would increase standard errors usually, but if x2 and x3 have high explanatory power, it could lower the SEs\n",
    "\n",
    "### c2\n",
    "housedata = woo.dataWoo('hprice1')\n",
    "\n",
    "# price                    house price, $1000s\n",
    "# bdrms                    number of bdrms\n",
    "# sqrft                    size of house in square feet\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = housedata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = housedata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(housedata)\n",
    "k = 1\n",
    "\n",
    "# extract y:\n",
    "y = housedata['price']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'sqrft': housedata['sqrft'], 'bdrms': housedata['bdrms']})\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='price ~ sqrft + bdrms', data=housedata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) price = -19.31 + 0.13*sqrft + 15.20*bdrms\n",
    "## (ii) holding square feet constant, one more bedroom increases the price of a house by $15,200\n",
    "## (iii) a house with an additional bedroom that is 140 square feet in size increases the price of a house by $33,400\n",
    "## (iv) 63.2% of the variation is price is explained by square footage and number of bedrooms\n",
    "## (v) the predicted selling price of a house with 2,438 square feet and 4 bedrooms is $362,330\n",
    "## (vi) the predicted selling price for the first house in the data is $358,430 so the buyer underpaid\n",
    "\n",
    "### c3\n",
    "ceosal2 = woo.dataWoo('ceosal2')\n",
    "\n",
    "y = ceosal2['lsalary']\n",
    "X = pd.DataFrame({'const' : 1, 'lsales': ceosal2['lsales'], 'lmktval': ceosal2['lmktval']})\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(177, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "reg = smf.ols(formula='lsalary ~ lsales + lmktval', data=ceosal2)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "y = ceosal2['lsalary']\n",
    "X = pd.DataFrame({'const' : 1, 'lsales': ceosal2['lsales'], 'lmktval': ceosal2['lmktval'], 'profits': ceosal2['profits']})\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(177, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "reg = smf.ols(formula='lsalary ~ lsales + lmktval + profits', data=ceosal2)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "y = ceosal2['lsalary']\n",
    "X = pd.DataFrame({'const' : 1, 'lsales': ceosal2['lsales'], 'lmktval': ceosal2['lmktval'], 'profits': ceosal2['profits'], 'ceoten': ceosal2['ceoten']})\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(177, 1)   # creates a column row\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "reg = smf.ols(formula='lsalary ~ lsales + lmktval + profits + ceoten', data=ceosal2)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4) })\n",
    "print(f'table: \\n{table}\\n')\n",
    "\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    " \n",
    "corr = np.corrcoef(ceosal2)\n",
    "print(corr)\n",
    "\n",
    "## (i) lsalary = 4.6209 + 0.1621*lsales + 0.1067*lmktval + e\n",
    "## (ii) there seems to be a multicollinearity problem between profits and sales; the R^2 is less than .3, which means not much variability in CEO salaries is explained by our independent variables\n",
    "## (iii) an additional year of CEO tenure will lead to 1.17% increase in salary\n",
    "\n",
    "### c5\n",
    "wagedata = woo.dataWoo('WAGE1')\n",
    "\n",
    "# wage                     average hourly earnings\n",
    "# educ                     years of education\n",
    "# exper                    years potential experience\n",
    "# tenure                   years with current employer\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = wagedata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = wagedata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(wagedata)\n",
    "k = 1\n",
    "\n",
    "# extract y:\n",
    "y = wagedata['educ']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'exper': wagedata['exper'], 'tenure': wagedata['tenure']})\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='educ ~ exper + tenure', data=wagedata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# obtain residuals as r_hat1\n",
    "r1_hat = results.resid\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(wage) ~ r1_hat', data=wagedata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(wage) ~ educ + exper + tenure', data=wagedata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) the r1_hat coefficient and the educ coefficient both have a value of 0.092\n",
    "\n",
    "### c6\n",
    "moneydata = woo.dataWoo('WAGE2')\n",
    "\n",
    "# wage                     monthly earnings\n",
    "# IQ                       IQ score\n",
    "# educ                     years of education\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = moneydata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = moneydata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(moneydata)\n",
    "k = 1\n",
    "\n",
    "# extract y:\n",
    "y = moneydata['IQ']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'educ': moneydata['educ']})\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='IQ ~ educ', data=moneydata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(wage) ~ educ', data=moneydata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(wage) ~ educ + IQ', data=moneydata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) S1_tilde is 3.5338\n",
    "## (ii) B1_tilde is 0.0598\n",
    "## (iii) B1_hat is 0.0391 and B2_hat is 0.0059\n",
    "## (iv) B1_hat + B2_hat*S1_tilde = B1_tilde >>> 0.0391 + 0.0059*3.5338 = 0.059 ≈ 0.058\n",
    "\n",
    "### c8\n",
    "fooddata = woo.dataWoo('discrim')\n",
    "\n",
    "# psoda                  price of medium soda, 1st wave\n",
    "# prpblck                proportion black, zipcode\n",
    "# income                 median family income, zipcode\n",
    "# prppov                 proportion in poverty, zipcode\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = fooddata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = fooddata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(fooddata)\n",
    "k = 1\n",
    "\n",
    "# extract y:\n",
    "y = fooddata['psoda']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'prpblck': fooddata['prpblck'], 'income': fooddata['income']})\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='psoda ~ prpblck + income', data=fooddata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='psoda ~ prpblck', data=fooddata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(psoda) ~ prpblck + np.log(income)', data=fooddata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='np.log(psoda) ~ prpblck + np.log(income) + prppov', data=fooddata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = fooddata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "## (i) the average value of prpblack is 0.11 and the average value of income is 47053.78 with respective units of measurement of proportion of black people (%) and median family income \n",
    "## (ii) psoda = 0.9563 + 0.1150prpblck + 1.60x10^-6income + u, sample size is 401, r-squared is 0.064\n",
    "##      the coefficient on prpblck means that as the proportion of black people in one zipcode increases by 1 unit, the price of a medium soda increases by 11.50 cents, i do not think it is economically large \n",
    "## (iii) new equations is psoda = 1.0374 + 0.0649prpblck + u, the discrimination effect is larger when you control for income\n",
    "## (iv) if prpblck increases by 0.20 or 20 percentage points, the estimated percentage change is 2.1216\n",
    "## (v) B(prpblck) decreases to 0.0728\n",
    "## (vi) the correlation between log(income) and prppov is -0.838467, this is logical because as the poverty in a zipcode increases one expects the median family income to decrease significantly \n",
    "## (vii) the statement is false because when determining the connection between price discrimination and black populations it is important to account for as many variables as possible, including income and poverty levels \n",
    "\n",
    "### c12\n",
    "econdata = woo.dataWoo('econmath')\n",
    "\n",
    "# score                     course score, in percent\n",
    "# acteng                    ACT English score\n",
    "# actmth                    ACT math score\n",
    "# colgpa                    college GPA, beginning semester\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = econdata.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = econdata.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(econdata)\n",
    "k = 1\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='score ~ colgpa + actmth + acteng', data=econdata)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) zero students received a perfect score, the average score is 72.60, the mean of actmth is 23.21, the mean of acteng is 22.59, the std of actmth is 3.77, the std of acteng is 3.79\n",
    "## (ii) colgpa = 16.17 + 12.37colgpa + 0.88actmth + 0.05acteng\n",
    "## (iii) math ACT score is a better predictor of performance because according to the coefficients, it has a greater effect on one's score by approximately 0.83\n",
    "## (iv) the r-squared is 0.397 meaning 39.7% of the variation in test scores is explained by college GPA, math ACT score, and english ACT score \n",
    "\n",
    "### c13\n",
    "gpa1data = woo.dataWoo('gpa1')\n",
    "\n",
    "# colGPA                   MSU GPA\n",
    "# hsGPA                    high school GPA\n",
    "# PC                       =1 of pers computer at sch\n",
    "# ACT                       'achievement' score\n",
    "# fathcoll                 =1 if father college grad\n",
    "# mothcoll                 =1 if mother college grad\n",
    "\n",
    "# find the summary statistics for all data \n",
    "Data_summary = gpa1data.describe()\n",
    "print(f'Data_summary:\\n{Data_summary}\\n')\n",
    "\n",
    "# find the correlation coreficients for all data \n",
    "Corr_coef = gpa1data.corr()\n",
    "print(f'Corr_coef:\\n{Corr_coef}\\n')\n",
    "\n",
    "# determine sample size & no. of regressors:\n",
    "n = len(gpa1data)\n",
    "k = 1\n",
    "\n",
    "# extract y:\n",
    "y = gpa1data['colGPA']\n",
    "\n",
    "# extract X & add a column of ones:\n",
    "X = pd.DataFrame({'const': 1, 'PC': gpa1data['PC']})\n",
    "\n",
    "# parameter estimates:\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1)  # creates a col vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# compute X'X, inv X'X, and show some properties\n",
    "XtX = X.T @ X\n",
    "print(f'XtX: \\n{XtX}\\n')\n",
    "\n",
    "invXtX = np.linalg.inv(XtX)\n",
    "print(f'invXtX: \\n{invXtX}\\n')\n",
    "\n",
    "invXtX_XtX = invXtX @ XtX\n",
    "print(f'invXtX_XtX: \\n{invXtX_XtX}\\n')\n",
    "\n",
    "# residuals, estimated variance of u and SER:\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimated variance of the parameter estimators and SE:\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='colGPA ~ PC', data=gpa1data)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='colGPA ~ PC + hsGPA + ACT', data=gpa1data)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# run regression using statsmodles and print full:\n",
    "reg = smf.ols(formula='colGPA ~ PC + fathcoll + mothcoll', data=gpa1data)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "## (i) b0 is 2.99 and b1 is 0.17\n",
    "##     b0 means that if one doesn't own a PC, they will still have a college GPA of 2.99, but if one does own a PC, their college GPA increases by 0.17\n",
    "## (ii) the coefficient on PC doesn't change much, but decreases slightly to 0.16\n",
    "##      B(hsGPA) makes sense because someone with a higher high school GPA, will probably also perform better in college than someone who struggles in high school academics\n",
    "## (iii) owning a PC is worth more than scoring 10 points higher on the ACT\n",
    "## (iv) once again, B1 on the variable PC does not change much and decreases to 0.16\n",
    "##      r-squared is 0.054, so i'm explaining 5.4% variation in college GPA\n",
    "## (v) i would respond that dropping one of the variables hsGPA or ACT because they are highly correlated will worsen my regression because i'm trying to account for as many variables that could affect colGPA as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29b589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
